{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report on Wrangled Data Collected For The Purpose of Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset that will be wrangled is the tweet archive of twitter user @dog_rates, also known as WeRateDogs. WeRateDogs is a twitter account that rates people's dogs with a humorous comment about the dog. The dataset that would be gathered are in three pieces, we have the twitter archived_enhanced csv file, the image prediction tsv file and an additional data from twitter api. After importing all the neccesary libraries the twitter archived_enhanced csv file was downloaded directly and loaded into a dataframe named df_archive with 2356 rows and 17 columns, the attribute of each row consist but not limited to the following tweet_id, timestamp, source, which indicates the platform the tweet was sent from, and text as the content of the tweet, some other attributes include rating_numerator, rating_denominator and name. The doggo, floofer, pupper, puppo attributes represent the dog stages. For the second piece of data the Requests library was used to download the tweet image prediction tsv file, the response from the request was converted to a string and encoded with utf-8 before loading the tsv file into a dataframe named df_pred with a 2075 rows and 12 columns, the attributes of each row contain tweet_id, jpg_url, img_num as the number of predicted images, p1 as first prediction, p2 as second prediction, p3 as third prediction and many more. The third piece of data can be gathered  via the Twitter API; twitter did not approve my reguest for api keys and authentication, I downloaded the data from the tweet-json text url, I made a directory to download tweet_json.txt into and write the json text file in the directory created above. I extracted the tweet_id, retweet_count and favorite_count from the response and appended to an empty list before passing the list to a dataframe named df_json which has 2354 rows and 3 columns, the attributes of the row are tweet_id, retweet_count and favorite_count.\n",
    "\n",
    "Accessing the three dataframes both visually and programmatically for cleaning, a number of quality and tidiness issues were encountered and documented below;\n",
    "\n",
    "- Retweets should not be included in the ratings this would be excluded from the analysis, also the column expanded_urls won't be necessary for our analysis hence we would drop it.\n",
    "\n",
    "- Five Columns in df_archive having more than 40% null values would be dropped, the columns affected are in_reply_to_status_id, in_reply_to_user_id, retweeted_status_id, retweeted_status_user_id, retweeted_status_timestamp.\n",
    "\n",
    "- The df_archive text column contain the main tweet, ratings and the tweet's link which would be split into different columns.\n",
    "\n",
    "- The values in column named source in df_archive should explicitly state the name of the platform the tweet was sent from, this would be replaced from html tag to the source name.\n",
    "\n",
    "- The rating_denominator and rating_numerator columns would be an extra columns since ratings would be extracted from the text column, as such both column wuld be dropped.\n",
    "\n",
    "- The dog with none as name could be explicitly stated for clear and concise meaning, the none value would be replaced with unknown.\n",
    "\n",
    "- The dog with no stage description in the doggo, floofer, pupper and puppo columns should be explicitly stated as unidentified.\n",
    "\n",
    "- The name 'a' ascribed to some of dogs in the name column of df_archive is not descriptive enough, I assume there might actually be dogs named such name, It will be a good practice to capitalize the letter.\n",
    "\n",
    "- The column tweet_id data type should have proper data type and as such would be change from int to string in all the dataframes, timestamp column in the twitter archive dataframe should be converted from object data type to datetime, the columns source, tweet, and name in the archive dataframe would be converted from object data type to string.\n",
    "\n",
    "- The prediction 1, 2 and 3 columns would be change from object to categorical variable.\n",
    "\n",
    "- The various stages of dog column doggo, floofer, pupper and puppo would be change from object data type to categorical variable.\n",
    "\n",
    "- The img_num column in the image prediction dataframe which indicates the number of images predicted should be in categorical column, this would be converted from int to categorical variable.\n",
    "\n",
    "- The p1, p1_conf, p1_dog, p2, p2_conf, p2_dog, p3, p3_conf, img_num, and p3_dog column labels in the image prediction dataframe should have a clear and precise meaning, these labels would be replaced.\n",
    "\n",
    "In cleaning the documented issues, the define, code and test framework was used, here the issue is defined, then followed by the code to solve the issue before finally testing and accessing the changes made. Before Exploratory Analysis and visualization the three dataframes were merge into a single dataframe on the tweet_id as a primary key, the merge was set to be inner merge to match all the records in the three dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
