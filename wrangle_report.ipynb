{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report on Wrangled Data Collected For The Purpose of Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wrangled dataset is that of a tweet archive of twitter user @dog_rates, also known as WeRateDogs. WeRateDogs is a twitter account that rates people's dogs with a humorous comment about the dog. The dataset that was gathered are in three pieces, we have the twitter archived_enhanced csv file, the image prediction tsv file and an additional data from twitter api. After importing all the neccesary libraries the twitter archived_enhanced csv file was downloaded directly and loaded into a dataframe named df_archive with 2356 rows and 17 columns, the attribute of each row consist but not limited to the following tweet_id, timestamp, source, which indicates the platform the tweet was sent from, and text as the content of the tweet, some other attributes include rating_numerator, rating_denominator and name. The doggo, floofer, pupper, puppo attributes represent the dog stages. For the second piece of data the Requests library was used to download the tweet image prediction tsv file, the response from the request was converted to a string and encoded with utf-8 before loading the tsv file into a dataframe named df_pred with 2075 rows and 12 columns, the attributes of each row contain tweet_id, jpg_url, img_num as the number of predicted images, p1 as first prediction, p2 as second prediction, p3 as third prediction and many more. The third piece of data could be gathered via the Twitter API; twitter did not approve my reguest for api keys and authentication, I downloaded the data from the tweet-json text url into directory into which the json text file would be written. Each tweet's JSON data was written to its own line. I extracted the tweet_id, retweet_count and favorite_count from the response and appended it to an empty list before passing the list to a dataframe named df_json which has 2354 rows and 3 columns, the attributes of the row are tweet_id, retweet_count and favorite_count.\n",
    "\n",
    "While assessing the three dataframes both visually and programmatically for cleaning, a number of quality and tidiness issues were encountered and documented below;\n",
    "\n",
    "### Quality issues\n",
    "- 1. Retweets should not be included in the ratings this would be excluded from the analysis, also the column expanded_urls won't be necessary for our analysis hence we would drop it. \n",
    "\n",
    "\n",
    "- 2. Five Columns in df_archive having more than 40% null values would be dropped, the columns affected are in_reply_to_status_id, in_reply_to_user_id, retweeted_status_id, retweeted_status_user_id,  retweeted_status_timestamp.               \n",
    "\n",
    "\n",
    "- 3. The df_archive text column contain the main tweet, ratings and the tweet's link which would be split into different columns.\n",
    "\n",
    "\n",
    "\n",
    "- 4. The values in column named source in df_archive should explicitly state the name of the platform the tweet was sent from, this would be replaced from html tag to the source name.\n",
    "\n",
    "\n",
    "\n",
    "- 5. The rating_denominator and rating_numerator columns would be an extra columns since ratings would be extracted from the text column, as such both column wuld be dropped. \n",
    "\n",
    "\n",
    "\n",
    "- 6. The dog with none as name could be explicitly stated for clear and concise meaning, the none value would be replaced with unknown.\n",
    "\n",
    "\n",
    "\n",
    "- 7. The dog with no stage value in the doggo, floofer, pupper and puppo columns should be explicitly stated as np.nan before merging the columns to a single colmn dog stage.\n",
    "\n",
    "\n",
    "- 8. The name 'a', 'an' and 'the' ascribed to some of dogs in the name column of df_archive is not descriptive enough, I assume these names are invalid for a dog, It will be a good to do away with the invalid dog name. \n",
    "\n",
    "\n",
    "- 9. The column tweet_id data type should have proper data type and as such would be change from int to string in all the dataframes, timestamp column in the twitter archive dataframe should be converted from object data type to datetime, the columns source, tweet, and name  in the archive dataframe would be converted from object data type to string.\n",
    "\n",
    "\n",
    "- 10. The prediction 1, 2 and 3 columns would be change from object to categorical variable.\n",
    "\n",
    "\n",
    "- 11. The various stages of dog column doggo, floofer,\tpupper and puppo would be change from object data type to categorical variable.\n",
    "\n",
    "\n",
    "- 12. The img_num column in the image prediction dataframe image number that corresponded to the most confident prediction (numbered 1 to 4 since tweets can have up to four images), this would be converted from int to categorical variable.\n",
    "\n",
    "\n",
    "- 13. p1, p1_conf, p1_dog, p2, p2_conf, p2_dog, p3, p3_conf, and p3_dog column labels in the image prediction dataframe should have a clear and precise meaning, these labels would be replaced.\n",
    "\n",
    "\n",
    "\n",
    "### Tidiness issues\n",
    "1. There are multiple dog stages columns present i.e. doggo, floofer, pupper and puppo. each observation should form a row, merging the columns to a single dog stage column would solve the issue, lastly the four dog stage columns would be dropped .\n",
    "\n",
    "2. All the three datasets are part of same observational unit.  merging 3 tables into one will done to tidy up the dataset.ables into one will done to tidy up the dataset.\n",
    "\n",
    "\n",
    "In cleaning the documented issues, the define, code and test framework was impleted, here the issue was defined, then followed by the code to solve the issue before finally testing and accessing the changes made. Before carrying out exploratory analysis and visualization the three dataframes were merge into a single dataframe on the tweet_id as a primary key, the merge was set to be inner merge to match all the records in the three dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
